{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Food_Image_Classification.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"D_a2USyd4giE"},"source":["# **Convolutional Neural Network**\n"]},{"cell_type":"code","metadata":{"id":"zhzdomRTOKoJ"},"source":["!gdown --id '1wCdNcClcd2p5UeDi6XxNdiBedNaVn3fP' --output food-11.zip # 下載資料集\n","!unzip food-11.zip # 解壓縮  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9sVrKci4PUFW"},"source":["# Import需要的套件\n","import os\n","import numpy as np\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import pandas as pd\n","from torch.utils.data import DataLoader, Dataset\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F0i9ZCPrOVN_"},"source":["#Read image\n","利用 OpenCV (cv2) 讀入照片並存放在 numpy array 中"]},{"cell_type":"code","metadata":{"id":"Zf7QPifJQNUK"},"source":["def readfile(path, label):\n","    # label 是一個 boolean variable，代表需不需要回傳 y 值\n","    image_dir = sorted(os.listdir(path))\n","    x = np.zeros((len(image_dir), 128, 128, 3), dtype=np.uint8)\n","    y = np.zeros((len(image_dir)), dtype=np.uint8)\n","    for i, file in enumerate(image_dir):\n","        img = cv2.imread(os.path.join(path, file))\n","        x[i, :, :] = cv2.resize(img,(128, 128))\n","        if label:\n","          y[i] = int(file.split(\"_\")[0])\n","    if label:\n","      return x, y\n","    else:\n","      return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ebVIY5HQQH7","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"9b48e1c3-703e-4985-aa2b-f345b1facfad"},"source":["#分別將 training set、validation set、testing set 用 readfile 函式讀進來\n","workspace_dir = './food-11'\n","print(\"Reading data\")\n","train_x, train_y = readfile(os.path.join(workspace_dir, \"training\"), True)\n","print(\"Size of training data = {}\".format(len(train_x)))\n","val_x, val_y = readfile(os.path.join(workspace_dir, \"validation\"), True)\n","print(\"Size of validation data = {}\".format(len(val_x)))\n","test_x = readfile(os.path.join(workspace_dir, \"testing\"), False)\n","print(\"Size of Testing data = {}\".format(len(test_x)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading data\n","Size of training data = 9866\n","Size of validation data = 3430\n","Size of Testing data = 3347\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gq5KVMM3OHY6"},"source":["# Dataset\n"]},{"cell_type":"code","metadata":{"id":"gKd2abixQghI"},"source":["#training 時做 data augmentation\n","train_transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.RandomHorizontalFlip(), #隨機將圖片水平翻轉\n","    transforms.RandomRotation(15), #隨機旋轉圖片\n","    transforms.ColorJitter(brightness = 0.1, contrast = 0.1, saturation = 0.1, hue = 0.1), # 改變圖片色調\n","    transforms.ToTensor(), #將圖片轉成 Tensor，並把數值normalize到[0,1](data normalization)\n","]) # 把要做的transform包在一起\n","#testing 時不需做 data augmentation\n","test_transform = transforms.Compose([\n","    transforms.ToPILImage(),                                    \n","    transforms.ToTensor(),\n","])\n","class ImgDataset(Dataset): # 繼承了Dataset\n","    def __init__(self, x, y=None, transform=None):\n","        self.x = x\n","        # label is required to be a LongTensor\n","        self.y = y\n","        if y is not None:\n","            self.y = torch.LongTensor(y)\n","        self.transform = transform\n","    def __len__(self):\n","        return len(self.x)\n","    def __getitem__(self, index):\n","        X = self.x[index]\n","        if self.transform is not None:\n","            X = self.transform(X)\n","        if self.y is not None:\n","            Y = self.y[index]\n","            return X, Y\n","        else:\n","            return X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qz6jeMnkQl0_"},"source":["batch_size = 128\n","train_set = ImgDataset(train_x, train_y, train_transform)\n","val_set = ImgDataset(val_x, val_y, test_transform)\n","train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j9YhZo7POPYG"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"Y1c-GwrMQqMl"},"source":["class Classifier(nn.Module):\n","    def __init__(self):\n","        super(Classifier, self).__init__()\n","        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n","        #torch.nn.MaxPool2d(kernel_size, stride, padding)\n","        #input 維度 [3, 128, 128]\n","        self.cnn = nn.Sequential(\n","            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]\n","            nn.BatchNorm2d(64),\n","            #nn.Dropout2d(0.25),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n","\n","            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n","\n","            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n","\n","            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n","            \n","            nn.Conv2d(512, 1024, 3, 1, 1), # [1024, 8, 8]\n","            nn.BatchNorm2d(1024),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),       # [1024, 4, 4]\n","\n","            nn.Conv2d(1024, 512, 3, 1, 1), # [512, 4, 4]\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),       # [512, 2, 2]\n","        )\n","        self.fc = nn.Sequential(\n","            nn.Linear(512*2*2, 1024),\n","            #nn.Dropout(0.25),\n","            nn.ReLU(),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 11)\n","        )\n","    def forward(self, x):\n","        out = self.cnn(x)\n","        out = out.view(out.size()[0], -1)\n","        return self.fc(out)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aEnGbriXORN3"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"_5x-FH2Kr_jh"},"source":["使用training set訓練，並使用validation set尋找好的參數"]},{"cell_type":"code","metadata":{"id":"PHaFE-8oQtkC","colab":{"base_uri":"https://localhost:8080/","height":340},"outputId":"dcb0aae8-8685-40ec-d612-2a7e7d66ceff"},"source":["model = Classifier().cuda()\n","loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer 使用 Adam\n","# 在影像辨識上 SGD + Momentum 的表現通常比 Adam 好\n","num_epoch = 30\n","\n","for epoch in range(num_epoch):\n","    epoch_start_time = time.time()\n","    train_acc = 0.0\n","    train_loss = 0.0\n","    val_acc = 0.0\n","    val_loss = 0.0\n","\n","    model.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n","    for i, data in enumerate(train_loader):\n","        optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n","        train_pred = model(data[0].cuda()) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n","        batch_loss = loss(train_pred, data[1].cuda()) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n","        batch_loss.backward() # 利用 back propagation 算出每個參數的 gradient\n","        optimizer.step() # 以 optimizer 用 gradient 更新參數值\n","        print(train_pred)\n","        print(train_pred.shape)\n","        print(data[1])\n","        print(data[1].shape)\n","        break\n","\n","        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n","        train_loss += batch_loss.item()\n","    \n","    model.eval()\n","    with torch.no_grad():\n","        for i, data in enumerate(val_loader):\n","            val_pred = model(data[0].cuda())\n","            batch_loss = loss(val_pred, data[1].cuda())\n","\n","            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n","            val_loss += batch_loss.item()\n","\n","        #將結果 print 出來\n","        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n","            (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n","             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))\n","    break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[-0.1359, -0.0973,  0.1542,  ...,  0.1082, -0.1881,  0.1155],\n","        [-0.2284, -0.0867,  0.1552,  ..., -0.0445, -0.2068,  0.0539],\n","        [-0.1718, -0.1641,  0.1454,  ...,  0.0070, -0.1701,  0.0737],\n","        ...,\n","        [-0.1601, -0.1797,  0.1616,  ...,  0.0767, -0.2360,  0.0532],\n","        [-0.1655, -0.1881,  0.0545,  ..., -0.0655, -0.2092, -0.0081],\n","        [-0.0852, -0.1376,  0.1911,  ...,  0.0556, -0.1233, -0.0367]],\n","       device='cuda:0', grad_fn=<AddmmBackward>)\n","torch.Size([128, 11])\n","tensor([ 8,  4,  8,  2,  2,  9,  9, 10,  9,  6,  3,  5,  9,  5,  1,  8,  0,  2,\n","         4,  8,  5,  1,  3,  2,  5,  8,  9,  1,  4,  2,  0,  0,  2,  9,  4,  9,\n","         8,  5,  9,  2,  8,  9,  0,  6,  8,  8,  5, 10,  8,  8,  5,  0,  8,  0,\n","         2,  0,  3,  9,  5,  2,  6,  1,  9,  2,  3,  3,  2,  8,  1, 10, 10,  8,\n","         0,  9,  2,  3,  2,  3,  2,  4,  2,  4,  3,  3,  6, 10,  8,  5,  5,  4,\n","         9,  9,  9,  8, 10,  3,  0,  9,  5,  3,  0,  2,  2,  9,  8,  0,  2,  7,\n","         5,  5, 10,  2,  9,  1,  9,  3,  2,  0,  4,  2,  9,  9, 10,  2, 10,  8,\n","         3,  2])\n","torch.Size([128])\n","[001/030] 6.51 sec(s) Train Acc: 0.000000 Loss: 0.000000 | Val Acc: 0.145773 loss: 0.018662\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"01yLMuUd9qu1"},"source":["# Confusion matrix"]},{"cell_type":"code","metadata":{"id":"qZBt4WZNkb1i"},"source":["from sklearn.metrics import confusion_matrix\n","\n","val_set = ImgDataset(val_x, transform=test_transform)\n","val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UIowZquV9tbn"},"source":["model.eval()\n","prediction = []\n","with torch.no_grad():\n","    for i, data in enumerate(val_loader):\n","        val_pred = model(data.cuda())\n","        val_label = np.argmax(val_pred.cpu().data.numpy(), axis=1) # 回傳最大值的index\n","        for y in val_label:\n","            prediction.append(y)\n","prediction = np.asarray(prediction)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ND9JtRbv9x0u"},"source":["cm = confusion_matrix(val_y, prediction)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hSqOrEUAnRjR","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a4614bb8-2742-4be0-e4e7-245c2581bc16"},"source":["len(prediction)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3430"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"Z-gFBEQpqdFV","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"ac03fae7-3d96-443b-a038-999cfd1b181f"},"source":["cm # 上方標籤為實際分類，左方標籤為預測分類"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 64,  13,  42,  28,  26,  82,   2,   2,  51,  41,  11],\n","       [  1,  50,  46,   0,   3,   6,   0,   3,  11,  23,   1],\n","       [  6,  14, 254,   3,  10,  96,   0,   0,  55,  52,  10],\n","       [  7,   8,  31, 137,   9,  39,   1,   1,  54,  35,   5],\n","       [  0,   2,  43,   1, 168,  45,   0,   4,  41,  18,   4],\n","       [  0,   1,  13,   1,   7, 376,   0,   0,  39,   6,   6],\n","       [  3,   0,   2,   3,   1,  21,  62,   5,   3,  39,   8],\n","       [  4,   2,   4,   0,   2,   1,  10,  49,   5,   6,  13],\n","       [  0,   2,  22,   1,   3,  25,   0,   0, 274,  11,   9],\n","       [  0,   5,  14,   2,   0,   6,   0,   0,  14, 459,   0],\n","       [  0,   4,   5,   0,   0,   9,   0,   0,  17,   1, 196]])"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"RKoUxLun8lFG"},"source":["train_val_x = np.concatenate((train_x, val_x), axis=0)\n","train_val_y = np.concatenate((train_y, val_y), axis=0)\n","train_val_set = ImgDataset(train_val_x, train_val_y, train_transform)\n","train_val_loader = DataLoader(train_val_set, batch_size=batch_size, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OoAS5TtRsfOo"},"source":["model_best = Classifier().cuda()\n","loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n","optimizer = torch.optim.Adam(model_best.parameters(), lr=0.001) # optimizer 使用 Adam\n","num_epoch = 120\n","\n","for epoch in range(num_epoch):\n","    epoch_start_time = time.time()\n","    train_acc = 0.0\n","    train_loss = 0.0\n","\n","    model_best.train()\n","    for i, data in enumerate(train_val_loader):\n","        optimizer.zero_grad()\n","        train_pred = model_best(data[0].cuda())\n","        batch_loss = loss(train_pred, data[1].cuda())\n","        batch_loss.backward()\n","        optimizer.step()\n","\n","        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n","        train_loss += batch_loss.item()\n","\n","        #將結果 print 出來\n","    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n","      (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n","      train_acc/train_val_set.__len__(), train_loss/train_val_set.__len__()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ek5aF8totfDN"},"source":["# Apex"]},{"cell_type":"code","metadata":{"id":"p3QbK6ubth1a","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1a66ded2-a56c-4c0d-c31b-573c3cae3f16"},"source":["%%writefile setup.sh\n","# 嘗試混合精度訓練\n","git clone https://github.com/NVIDIA/apex\n","cd apex\n","pip install -v --no-cache-dir ./"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing setup.sh\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zbi8Dgu3tlRE"},"source":["!sh setup.sh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1TGB8x-J3OVs"},"source":["batch_size = 128\n","train_set = ImgDataset(train_x, train_y, train_transform)\n","val_set = ImgDataset(val_x, val_y, test_transform)\n","train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBDRLyUHtoJk"},"source":["from apex import amp\n","model = Classifier().cuda()\n","loss_func = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer 使用 Adam\n","num_epoch = 30\n","model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") # O1指定混合精度訓練\n","# 用fp16取代float可以減少儲存空間並加速訓練，然而fp16的範圍比float狹窄許多，尤其容易出現underflow的問題，因此做動態損失放大 scale_loss \n","# 然而放大損失的過程中亦可能有overflow的問題，此時的做法是跳過目前步驟，選擇較小的放大倍數在執行一次\n","# Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0 \n","# 訓練初期容易有此問題，因為apex會嘗試較大的放大倍數\n","for epoch in range(num_epoch):\n","    epoch_start_time = time.time()\n","    train_acc = 0.0\n","    train_loss = 0.0\n","    val_acc = 0.0\n","    val_loss = 0.0\n","    model.train() # 確保 model 是在 train model (開啟 Dropout 等...)    \n","    for i, data in enumerate(train_loader):\n","        optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n","        train_pred = model(data[0].cuda()) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n","        batch_loss = loss_func(train_pred, data[1].cuda())\n","        with amp.scale_loss(batch_loss, optimizer) as scaled_loss:\n","          scaled_loss.backward()\n","        optimizer.step()\n","        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n","        train_loss += batch_loss.item()\n","    model.eval()\n","    with torch.no_grad():\n","        for i, data in enumerate(val_loader):\n","            val_pred = model(data[0].cuda())\n","            batch_loss = loss(val_pred, data[1].cuda())\n","            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n","            val_loss += batch_loss.item()\n","        #將結果 print 出來\n","        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n","            (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n","             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2o1oCMXy61_3"},"source":["# Testing"]},{"cell_type":"code","metadata":{"id":"iAR6sn8U661G"},"source":["test_set = ImgDataset(test_x, transform=test_transform)\n","test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4HznI9_-ocrq"},"source":["model_best.eval()\n","prediction = []\n","with torch.no_grad():\n","    for i, data in enumerate(test_loader):\n","        test_pred = model_best(data.cuda())\n","        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n","        for y in test_label:\n","            prediction.append(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_iKe_1nL8OBD"},"source":["# Saving model and results"]},{"cell_type":"code","metadata":{"id":"3t2q2Th85ZUE"},"source":["#將結果寫入 csv 檔\n","with open(\"predict.csv\", 'w') as f:\n","    f.write('Id,Category\\n')\n","    for i, y in  enumerate(prediction):\n","        f.write('{},{}\\n'.format(i, y))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yoVRAc68E_Uu"},"source":["from google.colab import files\n","files.download('predict.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BkHBeYEjiO-P","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"ade925f2-8999-4d92-dc37-3486e73eff6a"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hQUoF1HBidnm"},"source":["model_save_name = 'classifier.pt' # 107 epoch\n","path = F\"/content/gdrive/My Drive/{model_save_name}\" \n","torch.save(model_best.state_dict(), path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xIhdn3dE8U-Z"},"source":["# Downloading model"]},{"cell_type":"code","metadata":{"id":"TWkYCJhKisRL","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"aec7dda0-3e0e-4f8b-9096-829088b9fdd9"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"asiwdODpi66r"},"source":["model_best = Classifier().cuda()\n","loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n","optimizer = torch.optim.Adam(model_best.parameters(), lr=0.001) # optimizer 使用 Adam\n","num_epoch = 53"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YfiJUlAkjJpO","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"04984177-8236-4c46-b01d-bb1047e5fcdd"},"source":["path = \"/content/gdrive/My Drive/classifier.pt\"\n","model_best.load_state_dict(torch.load(path))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"JvLNH7xs8YHA"},"source":["# Keep training"]},{"cell_type":"code","metadata":{"id":"VT62UVcGjYS3","colab":{"base_uri":"https://localhost:8080/","height":697},"outputId":"b9ab18cf-5c17-4d66-9aac-8536432000bf"},"source":["# 正確率爬不上去時修改learning rate，加速訓練使training accuracy為1\n","optimizer = torch.optim.Adam(model_best.parameters(), lr=0.0001) # optimizer 使用 Adam\n","num_epoch = 40\n","for epoch in range(num_epoch):\n","    epoch_start_time = time.time()\n","    train_acc = 0.0\n","    train_loss = 0.0\n","\n","    model_best.train()\n","    for i, data in enumerate(train_val_loader):\n","        optimizer.zero_grad()\n","        train_pred = model_best(data[0].cuda())\n","        batch_loss = loss(train_pred, data[1].cuda())\n","        batch_loss.backward()\n","        optimizer.step()\n","\n","        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n","        train_loss += batch_loss.item()\n","\n","        #將結果 print 出來\n","    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n","      (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n","      train_acc/train_val_set.__len__(), train_loss/train_val_set.__len__()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[001/040] 56.72 sec(s) Train Acc: 0.995337 Loss: 0.000114\n","[002/040] 57.02 sec(s) Train Acc: 0.997443 Loss: 0.000054\n","[003/040] 57.32 sec(s) Train Acc: 0.998270 Loss: 0.000032\n","[004/040] 57.61 sec(s) Train Acc: 0.999323 Loss: 0.000023\n","[005/040] 57.84 sec(s) Train Acc: 0.999248 Loss: 0.000018\n","[006/040] 57.59 sec(s) Train Acc: 0.999173 Loss: 0.000016\n","[007/040] 57.58 sec(s) Train Acc: 0.999474 Loss: 0.000016\n","[008/040] 57.65 sec(s) Train Acc: 0.999398 Loss: 0.000011\n","[009/040] 57.67 sec(s) Train Acc: 0.999474 Loss: 0.000010\n","[010/040] 57.49 sec(s) Train Acc: 0.999398 Loss: 0.000013\n","[011/040] 57.78 sec(s) Train Acc: 0.999774 Loss: 0.000008\n","[012/040] 57.61 sec(s) Train Acc: 1.000000 Loss: 0.000004\n","[013/040] 57.62 sec(s) Train Acc: 0.999774 Loss: 0.000006\n","[014/040] 57.67 sec(s) Train Acc: 0.999699 Loss: 0.000006\n","[015/040] 57.64 sec(s) Train Acc: 0.999624 Loss: 0.000006\n","[016/040] 57.88 sec(s) Train Acc: 0.999774 Loss: 0.000011\n","[017/040] 57.55 sec(s) Train Acc: 0.999549 Loss: 0.000007\n","[018/040] 57.69 sec(s) Train Acc: 0.999774 Loss: 0.000007\n","[019/040] 57.82 sec(s) Train Acc: 0.999624 Loss: 0.000007\n","[020/040] 57.61 sec(s) Train Acc: 0.999925 Loss: 0.000004\n","[021/040] 57.58 sec(s) Train Acc: 0.999850 Loss: 0.000004\n","[022/040] 57.50 sec(s) Train Acc: 0.999850 Loss: 0.000004\n","[023/040] 57.31 sec(s) Train Acc: 0.999624 Loss: 0.000010\n","[024/040] 57.34 sec(s) Train Acc: 0.999398 Loss: 0.000017\n","[025/040] 57.41 sec(s) Train Acc: 0.999925 Loss: 0.000006\n","[026/040] 57.39 sec(s) Train Acc: 0.999925 Loss: 0.000003\n","[027/040] 57.65 sec(s) Train Acc: 0.999774 Loss: 0.000005\n","[028/040] 57.45 sec(s) Train Acc: 0.999774 Loss: 0.000005\n","[029/040] 57.50 sec(s) Train Acc: 0.999549 Loss: 0.000014\n","[030/040] 57.43 sec(s) Train Acc: 0.999699 Loss: 0.000004\n","[031/040] 57.31 sec(s) Train Acc: 0.999925 Loss: 0.000003\n","[032/040] 57.68 sec(s) Train Acc: 0.999474 Loss: 0.000007\n","[033/040] 57.27 sec(s) Train Acc: 1.000000 Loss: 0.000002\n","[034/040] 57.36 sec(s) Train Acc: 0.999925 Loss: 0.000002\n","[035/040] 57.41 sec(s) Train Acc: 1.000000 Loss: 0.000002\n","[036/040] 57.39 sec(s) Train Acc: 1.000000 Loss: 0.000001\n","[037/040] 57.61 sec(s) Train Acc: 0.999699 Loss: 0.000006\n","[038/040] 57.43 sec(s) Train Acc: 0.999699 Loss: 0.000007\n","[039/040] 57.56 sec(s) Train Acc: 0.999850 Loss: 0.000012\n","[040/040] 57.62 sec(s) Train Acc: 0.999850 Loss: 0.000004\n"],"name":"stdout"}]}]}